{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 케라스(keras)로 나만의 데이터셋을 이용하여 얼굴 인식 학습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_50 (Conv2D)           (None, 92, 92, 64)        4864      \n",
      "_________________________________________________________________\n",
      "conv2d_51 (Conv2D)           (None, 44, 44, 64)        102464    \n",
      "_________________________________________________________________\n",
      "conv2d_52 (Conv2D)           (None, 40, 40, 128)       204928    \n",
      "_________________________________________________________________\n",
      "conv2d_53 (Conv2D)           (None, 18, 18, 128)       409728    \n",
      "_________________________________________________________________\n",
      "conv2d_54 (Conv2D)           (None, 14, 14, 256)       819456    \n",
      "_________________________________________________________________\n",
      "conv2d_55 (Conv2D)           (None, 5, 5, 256)         1638656   \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                409664    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 3,590,410\n",
      "Trainable params: 3,590,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(64, (5, 5), strides=1, input_shape=(96, 96, 3)))\n",
    "model.add(layers.Conv2D(64, (5, 5), activation='relu', strides=2, ))\n",
    "model.add(layers.Conv2D(128, (5, 5), strides=1, ))\n",
    "model.add(layers.Conv2D(128, (5, 5), activation='relu', strides=2, ))\n",
    "model.add(layers.Conv2D(256, (5, 5), strides=1, ))\n",
    "model.add(layers.Conv2D(256, (5, 5), activation='relu', strides=2, ))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- layer (Conv2D와 MaxPooling2D)\n",
    "    - 입력\n",
    "        - input_shape(image_height, image_width, image_channels)\n",
    "    - 출력\n",
    "        - (height, width, channels) 크기의 3D 텐서를 return\n",
    "        - height와 width는 컨볼루션, stride의 영향으로 작아짐-\n",
    "        - channels는 입력에 의해\n",
    "- Dense\n",
    "    - 입력\n",
    "        - 1D의 형태이기 때문에, 위에서 만든 3D 텐서를 1D로 평평하게 해주어야 합니다.(Flatten)\n",
    "    - 출력\n",
    "        - 마지막 Dense Layer의 출력이 분류(Classification)하고자 하는 크기!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 만들기 (이미지 불러오기, 레이블 정보 만들어서 가져오기, 가공하기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'training_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-48088d9b23ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbatch_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mbatch_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-48088d9b23ba>\u001b[0m in \u001b[0;36mread_data_batch\u001b[0;34m(csv_file_name, batch_size)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_data_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0minput_queue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_queue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m#image,label,file_name= read_data(input_queue)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-48088d9b23ba>\u001b[0m in \u001b[0;36mget_input_queue\u001b[0;34m(csv_file_name, num_epochs)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m',|\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtrain_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'training_data.csv'"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 40\n",
    "\n",
    "def get_input_queue(csv_file_name, num_epochs = epochs):\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    for line in open(csv_file_name,'r'):\n",
    "        cols = re.split(',|\\n',line)\n",
    "        train_images.append(cols[0])\n",
    "        train_labels.append(int(cols[1]))\n",
    "    input_queue = tf.train.slice_input_producer([train_images,train_labels],\n",
    "                                               num_epochs = num_epochs,shuffle = True)\n",
    "    return input_queue\n",
    "\n",
    "def read_data(input_queue):\n",
    "    image_file = input_queue[0]\n",
    "    label = input_queue[1]\n",
    "    image =  tf.image.decode_jpeg(tf.read_file(image_file),channels=image_color)\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    label = tf.image.convert_image_dtype(label, dtype=tf.int64)\n",
    "    image.set_shape((96, 96, 3))\n",
    "    return image, label #,image_file\n",
    "\n",
    "def read_data_batch(csv_file_name, batch_size=batch_size):\n",
    "    input_queue = get_input_queue(csv_file_name)\n",
    "    image, label = read_data(input_queue)\n",
    "    #image,label,file_name= read_data(input_queue)\n",
    "\n",
    "    # random image\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image,max_delta=0.5)\n",
    "    image = tf.image.random_contrast(image,lower=0.2,upper=2.0)\n",
    "    image = tf.image.random_hue(image,max_delta=0.08)\n",
    "    image = tf.image.random_saturation(image,lower=0.2,upper=2.0)\n",
    "    # batch_image,batch_label,batch_file = tf.train.batch([image,label,file_name],batch_size=batch_size)\n",
    "        \n",
    "    batch_image,batch_label = tf.train.batch([image,label],batch_size=batch_size)\n",
    "    #,enqueue_many=True)\n",
    "    #batch_file = tf.reshape(batch_file,[batch_size,1])\n",
    "    #batch_label_on_hot=tf.one_hot(tf.to_int64(batch_label), num_classes, on_value=1.0, off_value=0.0)\n",
    "    #return batch_image, batch_label_on_hot#, batch_file\n",
    "    \n",
    "    return batch_image, batch_label\n",
    "\n",
    "batch_images, batch_labels = read_data_batch('training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
