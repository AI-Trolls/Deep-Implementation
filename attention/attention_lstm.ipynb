{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention implementation for Dense Layer in Keras v2\n",
    "\n",
    "reference: https://github.com/philipperemy/keras-attention-mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 2\n",
    "TIME_STEPS = 20\n",
    "SINGLE_ATTENTION_VECTOR = False # if True, the attention vector is shared across the input_dimentions where the attention is applied\n",
    "APPLY_ATTENTION_BEFORE_LSTM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_stamps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    #if SINGLE_ATTENTION_VECTOR:\n",
    "    #    a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "    #    a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    # output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    output_attention_mul = multiply([inputs, a_probs], name='attention_mul')\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_attention_applied_after_lstm():\n",
    "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "    lstm_units = 32\n",
    "    lstm_out = LSTM(lstm_units, return_sequences=True)(inputs)\n",
    "    attention_mul = attention_3d_block(lstm_out)\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    model = Model(inputs=[inputs], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_attention_applied_before_lstm():\n",
    "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "    attention_mul = attention_3d_block(inputs)\n",
    "    lstm_units = 32\n",
    "    attention_mul = LSTM(lstm_units, return_sequences=False)(attention_mul)\n",
    "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    model = Model(inputs=[inputs], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_recurrent(n, time_steps, input_dim, attention_column=10):\n",
    "    \"\"\"\n",
    "    Data generation. x is purely random except that it's first value equals the target y.\n",
    "    In practice, the network should learn that the target = x[attention_column].\n",
    "    Therefore, most of its attention should be focused on the value addressed by attention_column.\n",
    "    :param n: the number of samples to retrieve.\n",
    "    :param time_steps: the number of time steps of your series.\n",
    "    :param input_dim: the number of dimensions of each element in the series.\n",
    "    :param attention_column: the column linked to the target. Everything else is purely random.\n",
    "    :return: x: model inputs, y: model targets\n",
    "    \"\"\"\n",
    "    x = np.random.standard_normal(size=(n, time_steps, input_dim))\n",
    "    y = np.random.randint(low=0, high=2, size=(n, 1))\n",
    "    x[:, attention_column, :] = np.tile(y[:], (1, input_dim))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 300000\n",
    "inputs, outputs = get_data_recurrent(N, TIME_STEPS, INPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_18 (InputLayer)           (None, 20, 2)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_16 (LSTM)                  (None, 20, 32)       4480        input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "permute_11 (Permute)            (None, 32, 20)       0           lstm_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, 32, 20)       0           permute_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 32, 20)       420         reshape_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Permute)         (None, 20, 32)       0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_mul (Multiply)        (None, 20, 32)       0           lstm_16[0][0]                    \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 640)          0           attention_mul[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1)            641         flatten_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,541\n",
      "Trainable params: 5,541\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 270000 samples, validate on 30000 samples\n",
      "Epoch 1/1\n",
      "270000/270000 [==============================] - 67s 247us/step - loss: 0.0518 - acc: 0.9754 - val_loss: 1.0132e-04 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7c6a564a20>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    if APPLY_ATTENTION_BEFORE_LSTM:\n",
    "        m = model_attention_applied_before_lstm()\n",
    "    else:\n",
    "        m = model_attention_applied_after_lstm()\n",
    "        \n",
    "    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    print(m.summary())\n",
    "    \n",
    "    m.fit([inputs], outputs, epochs=1, batch_size=64, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(model, inputs, print_shape_only=False, layer_name=None):\n",
    "    #print('----- activations -----')\n",
    "    activations = []\n",
    "    input_shape = model.input\n",
    "\n",
    "    if layer_name is None:\n",
    "        outputs = [layer.output for layer in model.layers]\n",
    "    else:\n",
    "        outputs = [layer.output for layer in model.layers if layer.name == layer_name] # all layer outputs\n",
    "\n",
    "    funcs = [K.function([input_shape] + [K.learning_phase()], [out]) for out in outputs] # evaluation functions\n",
    "    layer_outputs = [func([inputs, 1.])[0] for func in funcs]\n",
    "    for layer_activations in layer_outputs:\n",
    "        activations.append(layer_activations)\n",
    "        #if print_shape_only:\n",
    "        #    print(layer_activations.shape)\n",
    "        #else:\n",
    "        #    print(layer_activations)\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_vectors = []\n",
    "for i in range(300):\n",
    "    testing_inputs, testing_outputs = get_data_recurrent(1, TIME_STEPS, INPUT_DIM)\n",
    "    attention_vector = np.mean(get_activations(m,\n",
    "                                             testing_inputs,\n",
    "                                             print_shape_only=True,\n",
    "                                             layer_name='attention_vec')[0],\n",
    "                               axis=2).squeeze()\n",
    "    #print('attention =', attention_vector)\n",
    "    assert (np.sum(attention_vector) - 1.0) < 1e-5\n",
    "    attention_vectors.append(attention_vector)\n",
    "    \n",
    "attention_vector_final = np.mean(np.array(attention_vectors), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(attention_vector_final,\n",
    "             columns=['attention (%)']).plot(kind='bar',\n",
    "                                             title='Attention Mechanism as '\n",
    "                                             'a function of input'\n",
    "                                             ' dimensions.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
